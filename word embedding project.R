# ============================================================
# Word Embeddings via Truncated SVD
# Author: Chao Wang
# Description:
#   This script constructs interpretable word embeddings from
#   a word co-occurrence matrix using truncated SVD.
#   The embeddings are evaluated through similarity search,
#   analogy tasks, and bias analysis.
# ============================================================

# -----------------------
# Libraries
# -----------------------
library(tidyverse)
library(RSpectra)
library(ggrepel)

# -----------------------
# Data Loading
# -----------------------
dictionary <- scan("dictionary.txt", what = character(), sep = "\n")
M <- as.matrix(read.csv("co_occur.csv", header = FALSE))

# -----------------------
# Preprocessing
# -----------------------
# Log-normalize the co-occurrence matrix to reduce dominance
# of high-frequency word pairs
M_log <- log1p(M)

# -----------------------
# Truncated SVD
# -----------------------
k <- 100
svd_out <- svds(M_log, k = k)

# Right singular vectors represent word embeddings
V <- svd_out$v

# -----------------------
# Singular Value Diagnostics
# -----------------------
par(mar = c(3, 3, 2, 1))
plot(
  svd_out$d,
  type = "b",
  pch = 16,
  col = "steelblue",
  xlab = "Component Index",
  ylab = "Singular Value",
  main = "Top Singular Values of Log-Normalized Matrix"
)

# Interpretation:
# The rapid decay of singular values indicates that the
# co-occurrence matrix admits a low-rank approximation.

# -----------------------
# Interpreting Singular Directions
# -----------------------
interpret_vector <- function(v, dictionary, n = 10) {
  list(
    positive = dictionary[order(v, decreasing = TRUE)[1:n]],
    negative = dictionary[order(v, decreasing = FALSE)[1:n]]
  )
}

for (i in 1:5) {
  cat("\nSingular direction", i, "\n")
  res <- interpret_vector(V[, i], dictionary)
  cat("Positive terms:", paste(res$positive, collapse = ", "), "\n")
  cat("Negative terms:", paste(res$negative, collapse = ", "), "\n")
}

# Note:
# Not all singular directions admit clear semantic interpretation.
# Some capture noise or subtle distributional structure.

# -----------------------
# Normalize Word Embeddings
# -----------------------
V_norm <- V / sqrt(rowSums(V^2))

# -----------------------
# Gender Direction Analysis
# -----------------------
idx_woman <- which(dictionary == "woman")
idx_man   <- which(dictionary == "man")

gender_direction <- V_norm[idx_woman, ] - V_norm[idx_man, ]

project_onto <- function(emb, direction) {
  sum(emb * direction)
}

# -----------------------
# Projection: Family & Neutral Terms
# -----------------------
words_1 <- c("boy", "girl", "brother", "sister",
             "king", "queen", "he", "she",
             "john", "mary", "wall", "tree")

idx_1 <- match(words_1, dictionary)
proj_1 <- V_norm[idx_1, ] %*% gender_direction

df1 <- tibble(word = words_1, projection = proj_1)

ggplot(df1, aes(x = projection, y = 0, label = word)) +
  geom_point(size = 3, color = "steelblue") +
  geom_label_repel() +
  geom_vline(xintercept = 0, linetype = "dashed") +
  theme_minimal() +
  labs(
    x = "Projection onto Gender Direction",
    title = "Gender Projection of Selected Terms"
  )

# Interpretation:
# Words closer to male-associated terms project negatively,
# while female-associated terms project positively.
# Neutral terms lie near the origin.

# -----------------------
# Projection: Professions & Abstract Terms
# -----------------------
words_2 <- c("math", "matrix", "history", "nurse", "doctor",
             "pilot", "teacher", "engineer",
             "science", "arts", "literature",
             "bob", "alice")

idx_2 <- match(words_2, dictionary)
proj_2 <- V_norm[idx_2, ] %*% gender_direction

df2 <- tibble(word = words_2, projection = proj_2)

ggplot(df2, aes(x = projection, y = 0, label = word)) +
  geom_point(size = 3, color = "steelblue") +
  geom_label_repel() +
  geom_vline(xintercept = 0, linetype = "dashed") +
  theme_minimal() +
  labs(
    x = "Projection onto Gender Direction",
    title = "Gender Associations in Professional and Abstract Terms"
  )

# Interpretation:
# Observed associations reflect training data bias and
# historical co-occurrence patterns in text corpora.

# -----------------------
# Similarity Search
# -----------------------
cosine_similarity <- function(a, b) sum(a * b)

idx_montreal <- which(dictionary == "montreal")
sim_scores <- apply(V_norm, 1, cosine_similarity, b = V_norm[idx_montreal, ])

top_similar <- order(sim_scores, decreasing = TRUE)[1:10]

tibble(
  word = dictionary[top_similar],
  similarity = sim_scores[top_similar]
)

# -----------------------
# Analogy Evaluation
# -----------------------
analogy <- read.table("analogy_task.txt", stringsAsFactors = FALSE)

solve_analogy <- function(w1, w2, w3, dict, V) {
  if (!all(c(w1, w2, w3) %in% dict)) return(NA)
  
  target <- V[dict == w2, ] -
    V[dict == w1, ] +
    V[dict == w3, ]
  target <- target / sqrt(sum(target^2) + 1e-8)
  
  sims <- apply(V, 1, function(row) sum(row * target))
  sims[dict %in% c(w1, w2, w3)] <- -Inf
  
  dict[which.max(sims)]
}

predictions <- vector("character", nrow(analogy))
for (i in 1:nrow(analogy)) {
  predictions[i] <- solve_analogy(
    analogy[i, 1],
    analogy[i, 2],
    analogy[i, 3],
    dictionary,
    V_norm
  )
}

analogy_accuracy <- mean(predictions == analogy[, 4], na.rm = TRUE)

cat("Analogy task accuracy (k = 100):",
    round(analogy_accuracy, 3), "\n")

# Interpretation:
# The analogy task provides an intrinsic evaluation of embedding quality.
# With k = 100, the model achieves an accuracy of approximately 0.55,
# indicating that low-rank SVD captures meaningful semantic structure,
# though performance is sensitive to embedding dimensionality.


# -----------------------
# Effect of Embedding Dimensionality (Methodological Improvement)
# -----------------------
# Increasing the embedding dimension k allows the model to capture
# additional semantic structure at the cost of higher complexity.
# This experiment evaluates k as a tuning parameter to improve
# analogy task accuracy.

k_values <- c(50, 150, 200, 300, 400)

evaluate_k <- function(k, M_log, analogy, dictionary) {
  Vk <- svds(M_log, k = k)$v
  Vk <- Vk / sqrt(rowSums(Vk^2))
  
  preds <- vector("character", nrow(analogy))
  for (i in 1:nrow(analogy)) {
    preds[i] <- solve_analogy(
      analogy[i, 1],
      analogy[i, 2],
      analogy[i, 3],
      dictionary,
      Vk
    )
  }
  
  mean(preds == analogy[, 4], na.rm = TRUE)
}

accuracy_by_k <- sapply(
  k_values,
  evaluate_k,
  M_log = M_log,
  analogy = analogy,
  dictionary = dictionary
)

results_k <- data.frame(
  embedding_dimension = k_values,
  analogy_accuracy = round(accuracy_by_k, 3)
)

print(results_k)
# Interpretation:
# Analogy accuracy improves as embedding dimensionality increases,
# reaching a maximum of approximately 0.61 at k = 300.
# This suggests that selecting k is a critical methodological choice
# for balancing representational capacity and performance.
# Beyond k â‰ˆ 300, gains are marginal, indicating diminishing returns.

